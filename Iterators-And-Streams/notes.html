<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Working with Iterators and Streams</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 20px;
            background-color: #F4F4F4;
            color: #333;
        }
        h1, h2 {
            color: #2C3E50;
        }
        pre {
            background-color: #EAEAEA;
            padding: 10px;
            border-left: 5px solid #3498DB;
            overflow-x: auto;
        }
        footer {
            margin-top: 20px;
            font-size: 0.9em;
            color: #777;
        }
    </style>
</head>
<body>
    <h1>Working with Iterators and Streams</h1>

    <h2>Where I Struggled</h2>
    <p>
        Initially, understanding how iterators and generators work under the hood was tricky. Writing a custom iterator with <code>__iter__</code> and <code>__next__</code> seemed unnecessary at first, but it helped grasp how Python handles iteration internally.  
        File streaming was another challenge—handling large files efficiently while maintaining readability required switching from simple file reads to generator-based processing.
    </p>

    <h2>Breaking the Problem Down</h2>
    <p>
        The tasks varied in complexity, so I took a layered approach:
        <ul>
            <li>Started with basic iterator creation and moved to file streaming.</li>
            <li>Converted a custom file-reading iterator into a generator for efficiency.</li>
            <li>Built processing pipelines that applied multiple transformations dynamically.</li>
            <li>Used <code>itertools.chain</code> to combine multiple iterators seamlessly.</li>
            <li>Integrated exception handling to deal with file access issues.</li>
        </ul>
    </p>

    <h2>What Went Surprisingly Well</h2>
    <p>
        Implementing stream functions like line numbering and filtering was smoother than expected, thanks to Python’s built-in iterators.  
        Mapping existing string functions to stream functions helped reuse logic effectively. Also, using YAML configurations to define transformation pipelines made the system more flexible.
    </p>

    <h2>Key Learnings</h2>
    <p>
        - Generators significantly improve memory efficiency for large datasets.<br>
        - Using iterators allows for a functional, pipeline-style approach to processing data.<br>
        - Exception handling in streams is crucial to prevent silent failures in file processing.<br>
        - Writing reusable stream functions ensures flexibility when modifying pipelines later.<br>
    </p>

    <h2>What AI Helped Me</h2>
    <p>
        I used AI to clarify how iterators and generators differ in behavior, especially regarding memory usage. It also helped in designing function lookups for dynamic pipeline execution.  
        Official Python documentation on <code>itertools</code> and <code>fileinput</code> provided additional insights.
    </p>

    <footer>
        <p>Note: This document was created based on my experience solving the problem.</p>
    </footer>
</body>
</html>
